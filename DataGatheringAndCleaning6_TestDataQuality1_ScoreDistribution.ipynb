{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: TEST DATA QUALITY\n",
    "# Explore Result Quality Part II.\n",
    "# The results from before will be analyzed more structuredly\n",
    "# This notebook looks at the distribution of the score value in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Preparation\n",
    "# Import categorized 'names_cat.csv'\n",
    "import pandas\n",
    "\n",
    "print(\"Importing names... \")\n",
    "names = pandas.read_csv(\"data/names_cat_i2.csv\", usecols=[\"name\", \"n_publs\", \"likely_gender\", \"score\"])\n",
    "\n",
    "# Setting index & accessing cells: https://pythonhow.com/accessing-dataframe-columns-rows-and-cells/\n",
    "names = names.set_index(\"name\", drop = False)\n",
    "print(\"Names imported. They look like this: {}\".format(names[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "print(\"Importing names... \")\n",
    "names_old = pandas.read_csv(\"data/names_cat.csv\", usecols=[\"name\", \"n_publs\", \"likely_gender\", \"score\"])\n",
    "\n",
    "# Setting index & accessing cells: https://pythonhow.com/accessing-dataframe-columns-rows-and-cells/\n",
    "names_old = names_old.set_index(\"name\", drop = False)\n",
    "print(\"Names imported. They look like this: {}\".format(names_old[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = names[names['likely_gender'] == 'female']\n",
    "m = names[names['likely_gender'] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_old = names_old[names_old['likely_gender'] == 'female']\n",
    "m_old = names_old[names_old['likely_gender'] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bin sizes for histograms\n",
    "bins_f = f['score'].max()\n",
    "bins_m = m['score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bin sizes for histograms\n",
    "bins_f_old = f_old['score'].max()\n",
    "bins_m_old = m_old['score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f_old['score'].hist(density=\"True\", alpha=0.5, label=\"Female before\", bins=bins_f_old)\n",
    "f['score'].hist(density=\"True\", alpha=0.5, label=\"Female corrected\", bins=bins_f)\n",
    "\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Histogram: How did the scores improve for male classification (normalized)?\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_old['score'].hist(density=\"True\", alpha=0.5, label=\"Male before\", bins=bins_m_old)\n",
    "m['score'].hist(density=\"True\", alpha=0.5, label=\"Male corrected\", bins=bins_m)\n",
    "\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Histogram: How did the scores improve for male classification (normalized)?\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['score'].hist(density=\"True\", alpha=0.5, label=\"Female corrected\", bins=bins_f)\n",
    "m['score'].hist(density=\"True\", alpha=0.5, label=\"Male corrected\", bins=bins_m)\n",
    "\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Histogram: Which gender has which score how often (normalized)?\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean score of women has improved by {}.\".format(f['score'].mean() - f_old['score'].mean()))\n",
    "print(\"The median of the score score of women has improved by {}.\".format(f['score'].median() - f_old['score'].median()))\n",
    "print(\"The std of the score of women has changed by {}.\".format(f['score'].std() - f_old['score'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean score of men has improved by {}.\".format(m['score'].mean() - m_old['score'].mean()))\n",
    "print(\"The median of the score score of men has improved by {}.\".format(m['score'].median() - m_old['score'].median()))\n",
    "print(\"The std of the score of men has changed by {}.\".format(m['score'].std() - m_old['score'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Evaluation:\n",
    "# The scores are still not equally distributed. \n",
    "# The NamSor API endpoint that takes a parsed name could improve the results for scores that where previously 5 or lower,\n",
    "# so we see a considerable shift for the score distribution for women: Their scores are now way better!\n",
    "# Women's results improved more than men's results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
